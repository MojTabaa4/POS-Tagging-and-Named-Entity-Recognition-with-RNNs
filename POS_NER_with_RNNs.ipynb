{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MojTabaa4/POS-Tagging-and-Named-Entity-Recognition-with-RNNs/blob/main/POS_NER_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "5mz_Q8AgXQvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "V5ik_Yl1Coht"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from keras.layers import (GRU, LSTM, RNN, Bidirectional, Dense, Dropout,\n",
        "                          Embedding, Input, Masking, SimpleRNN,\n",
        "                          SpatialDropout1D, TimeDistributed)\n",
        "from keras.models import Model, Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PTBPosLoader:\n",
        "    \"\"\"\n",
        "    A class for loading and preprocessing the Penn Treebank Part of Speech dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, use_universal_tagset: bool = True, test_size: float = 0.1, val_size: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes the PTBPosLoader object.\n",
        "\n",
        "        Args:\n",
        "            use_universal_tagset (bool): Whether to use the universal tagset. Default is True.\n",
        "            test_size (float): The proportion of the dataset to use for testing. Default is 0.1.\n",
        "            val_size (float): The proportion of the training set to use for validation. Default is 0.1.\n",
        "        \"\"\"\n",
        "        if use_universal_tagset:\n",
        "            self.ptb = list(treebank.tagged_sents(tagset='universal'))\n",
        "        else:\n",
        "            self.ptb = list(treebank.tagged_sents())\n",
        "\n",
        "        self.test_size = test_size\n",
        "        self.val_size = val_size\n",
        "        self._split_train_val_test_sets()\n",
        "\n",
        "    def _split_train_val_test_sets(self) -> None:\n",
        "        \"\"\"\n",
        "        Splits the dataset into training, validation, and testing sets.\n",
        "        \"\"\"\n",
        "        self.train_set, self.test_set = train_test_split(self.ptb, \n",
        "                                                         test_size=self.test_size,\n",
        "                                                         random_state=100)\n",
        "        self.train_set, self.val_set = train_test_split(self.train_set,\n",
        "                                                        test_size=self.val_size,\n",
        "                                                        random_state=100)\n",
        "\n",
        "    def _extract_all_word_tag_pairs(self) -> None:\n",
        "        \"\"\"\n",
        "        Extracts all word-tag pairs from the training, validation, and testing sets.\n",
        "        \"\"\"\n",
        "        self.train_word_tag_pairs = [word_tag for record in self.train_set for word_tag in record]\n",
        "        self.val_word_tag_pairs = [word_tag for record in self.val_set for word_tag in record]\n",
        "        self.test_word_tag_pairs = [word_tag for record in self.test_set for word_tag in record]\n",
        "\n",
        "    def get_vocab_and_tagset(self) -> Tuple[set, List[str]]:\n",
        "        \"\"\"\n",
        "        Returns a tuple containing the vocabulary and tagset of the training set.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the vocabulary (set) and tagset (list of strings) of the training set.\n",
        "        \"\"\"\n",
        "        self._extract_all_word_tag_pairs()\n",
        "        vocab = set([word_tag[0] for word_tag in self.train_word_tag_pairs])\n",
        "        tagset = sorted(list(set([pair[1] for pair in self.train_word_tag_pairs])))\n",
        "        return vocab, tagset\n",
        "\n",
        "    def get_train_val_test_sets(self) -> Tuple[List[List[Tuple[str, str]]], List[List[Tuple[str, str]]], List[List[Tuple[str, str]]]]:\n",
        "        \"\"\"\n",
        "        Returns a tuple containing the training, validation, and testing sets.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the training, validation, and testing sets, each as a list of sentences, where each sentence \n",
        "            is a list of word-tag pairs (tuples).\n",
        "        \"\"\"\n",
        "        return self.train_set, self.val_set, self.test_set"
      ],
      "metadata": {
        "id": "PUDIJwNgVeon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4ZKH14JaPVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}