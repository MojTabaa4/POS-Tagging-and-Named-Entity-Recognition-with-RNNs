{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MojTabaa4/POS-Tagging-and-Named-Entity-Recognition-with-RNNs/blob/main/POS_NER_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "5mz_Q8AgXQvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "V5ik_Yl1Coht"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from keras.layers import (GRU, LSTM, RNN, Bidirectional, Dense, Dropout,\n",
        "                          Embedding, Input, Masking, SimpleRNN,\n",
        "                          SpatialDropout1D, TimeDistributed)\n",
        "from keras.models import Model, Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PTBPosLoader:\n",
        "    \"\"\"\n",
        "    A class for loading and preprocessing the Penn Treebank Part of Speech dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, use_universal_tagset: bool = True, test_size: float = 0.1, val_size: float = 0.1):\n",
        "        \"\"\"\n",
        "        Initializes the PTBPosLoader object.\n",
        "\n",
        "        Args:\n",
        "            use_universal_tagset (bool): Whether to use the universal tagset. Default is True.\n",
        "            test_size (float): The proportion of the dataset to use for testing. Default is 0.1.\n",
        "            val_size (float): The proportion of the training set to use for validation. Default is 0.1.\n",
        "        \"\"\"\n",
        "        if use_universal_tagset:\n",
        "            self.ptb = list(treebank.tagged_sents(tagset='universal'))\n",
        "        else:\n",
        "            self.ptb = list(treebank.tagged_sents())\n",
        "\n",
        "        self.test_size = test_size\n",
        "        self.val_size = val_size\n",
        "        self._split_train_val_test_sets()\n",
        "\n",
        "    def _split_train_val_test_sets(self) -> None:\n",
        "        \"\"\"\n",
        "        Splits the dataset into training, validation, and testing sets.\n",
        "        \"\"\"\n",
        "        self.train_set, self.test_set = train_test_split(self.ptb, \n",
        "                                                         test_size=self.test_size,\n",
        "                                                         random_state=100)\n",
        "        self.train_set, self.val_set = train_test_split(self.train_set,\n",
        "                                                        test_size=self.val_size,\n",
        "                                                        random_state=100)\n",
        "\n",
        "    def _extract_all_word_tag_pairs(self) -> None:\n",
        "        \"\"\"\n",
        "        Extracts all word-tag pairs from the training, validation, and testing sets.\n",
        "        \"\"\"\n",
        "        self.train_word_tag_pairs = [word_tag for record in self.train_set for word_tag in record]\n",
        "        self.val_word_tag_pairs = [word_tag for record in self.val_set for word_tag in record]\n",
        "        self.test_word_tag_pairs = [word_tag for record in self.test_set for word_tag in record]\n",
        "\n",
        "    def get_vocab_and_tagset(self) -> Tuple[set, List[str]]:\n",
        "        \"\"\"\n",
        "        Returns a tuple containing the vocabulary and tagset of the training set.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the vocabulary (set) and tagset (list of strings) of the training set.\n",
        "        \"\"\"\n",
        "        self._extract_all_word_tag_pairs()\n",
        "        vocab = set([word_tag[0] for word_tag in self.train_word_tag_pairs])\n",
        "        tagset = sorted(list(set([pair[1] for pair in self.train_word_tag_pairs])))\n",
        "        return vocab, tagset\n",
        "\n",
        "    def get_train_val_test_sets(self) -> Tuple[List[List[Tuple[str, str]]], List[List[Tuple[str, str]]], List[List[Tuple[str, str]]]]:\n",
        "        \"\"\"\n",
        "        Returns a tuple containing the training, validation, and testing sets.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the training, validation, and testing sets, each as a list of sentences, where each sentence \n",
        "            is a list of word-tag pairs (tuples).\n",
        "        \"\"\"\n",
        "        return self.train_set, self.val_set, self.test_set"
      ],
      "metadata": {
        "id": "PUDIJwNgVeon"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement vitebri algorithm"
      ],
      "metadata": {
        "id": "L9w-J-nGp5_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTagger:\n",
        "    def __init__(self, dataset_loader: PTBPosLoader, smoothing_const: float = 0.001) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the POSTagger.\n",
        "\n",
        "        Args:\n",
        "            dataset_loader: An instance of PTBPosLoader for loading the dataset.\n",
        "            smoothing_const: Smoothing constant for handling out-of-vocabulary words.\n",
        "        \"\"\"\n",
        "        self.dataset_loader = dataset_loader\n",
        "        self.smoothing_const = smoothing_const\n",
        "        self.emission_file_name = 'emission_ptb.csv'\n",
        "\n",
        "        self.train_set, self.val_set, self.test_set = self.dataset_loader.split_train_val_test_set()\n",
        "\n",
        "        self.train_words_tag, self.val_words_tag, self.test_words_tag = self.dataset_loader.extract_all_words_tag()\n",
        "        self.vocab, self.tagset = self.dataset_loader.set_vocab_and_tagset()\n",
        "        self.t = len(self.tagset)\n",
        "        self.v = len(self.vocab)\n",
        "\n",
        "        self.test_run_base: List[Tuple[str, str]] = [tup for sent in self.test_set for tup in sent]\n",
        "        self.test_tagged_words: List[str] = [tup[0] for sent in self.test_set for tup in sent]\n",
        "\n",
        "        self.tags_matrix: np.ndarray = None\n",
        "        self.trans_df: pd.DataFrame = None\n",
        "        self.emission_matrix: np.ndarray = None\n",
        "        self.emission_df: pd.DataFrame = None\n",
        "\n",
        "    def get_transition(self, curr_tag: str, prev_tag: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the transition probability between two tags.\n",
        "\n",
        "        Args:\n",
        "            curr_tag: Current tag.\n",
        "            prev_tag: Previous tag.\n",
        "\n",
        "        Returns:\n",
        "            Transition probability from prev_tag to curr_tag.\n",
        "        \"\"\"\n",
        "        count_prev_tag = len([tag for tag in self.all_tags if tag == prev_tag])\n",
        "        count_prev_tag_curr_tag = sum(\n",
        "            1 for i in range(len(self.all_tags) - 1)\n",
        "            if self.all_tags[i + 1] == curr_tag and self.all_tags[i] == prev_tag\n",
        "        )\n",
        "        return count_prev_tag_curr_tag / count_prev_tag\n",
        "\n",
        "    def set_transition_matrix(self, print_transition_matrix: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Set the transition matrix based on the training set.\n",
        "\n",
        "        Args:\n",
        "            print_transition_matrix: Whether to print the transition matrix.\n",
        "        \"\"\"\n",
        "        self.all_tags = [pair[1] for pair in self.train_words_tag]\n",
        "        self.tags_matrix = np.zeros((len(self.tagset), len(self.tagset)), dtype='float32')\n",
        "        for i, t1 in enumerate(list(self.tagset)):\n",
        "            for j, t2 in enumerate(list(self.tagset)):\n",
        "                self.tags_matrix[i, j] = self.get_transition(t2, t1)\n",
        "        self.trans_df = pd.DataFrame(self.tags_matrix, columns=list(self.tagset), index=list(self.tagset))\n",
        "        if print_transition_matrix:\n",
        "            print(self.trans_df)\n",
        "\n",
        "    def get_emission(self, word: str, tag: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the emission probability of a word given a tag.\n",
        "\n",
        "        Args:\n",
        "            word: Word.\n",
        "            tag: Tag.\n",
        "\n",
        "        Returns:\n",
        "            Emission probability of the word given the tag.\n",
        "        \"\"\"\n",
        "        pair_with_tag_t = [pair for pair in self.train_words_tag if pair[1] == tag]\n",
        "        word_with_tag_t = [pair[0] for pair in pair_with_tag_t if pair[0] == word[0]]\n",
        "        return len(word_with_tag_t) / len(pair_with_tag_t)\n",
        "\n",
        "    def set_emission_matrix(self, read_from_file: bool = True, print_emission_matrix: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Set the emission matrix based on the training or saved matrix.\n",
        "\n",
        "        Args:\n",
        "            read_from_file: Whether to read the emission matrix from a file.\n",
        "            print_emission_matrix: Whether to print the emission matrix.\n",
        "        \"\"\"\n",
        "        if read_from_file:\n",
        "            self.read_emission_csv()\n",
        "        else:\n",
        "            self.emission_matrix = np.zeros((len(self.test_words_tag), len(self.tagset)), dtype='float32')\n",
        "            for i, w in enumerate(list(self.test_words_tag)):\n",
        "                if i % 300 == 0:\n",
        "                    print(f\"round: {i}\")\n",
        "                for j, t2 in enumerate(list(self.tagset)):\n",
        "                    self.emission_matrix[i, j] = self.get_emission(w, t2)\n",
        "            self.emission_df = pd.DataFrame(self.emission_matrix, columns=list(self.tagset),\n",
        "                                            index=list(self.test_words_tag))\n",
        "\n",
        "        if print_emission_matrix:\n",
        "            print(self.emission_df)\n",
        "\n",
        "    def save_emission_csv(self) -> None:\n",
        "        \"\"\"Save the emission matrix to a CSV file.\"\"\"\n",
        "        self.emission_df.to_csv(self.emission_file_name, index=list(self.test_words_tag))\n",
        "\n",
        "    def read_emission_csv(self) -> None:\n",
        "        \"\"\"Read the emission matrix from a CSV file.\"\"\"\n",
        "        self.emission_df = pd.read_csv(self.emission_file_name).set_index('Unnamed: 0')\n",
        "\n",
        "    def _viterbi(self, read_emission_file: bool = True) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Perform the Viterbi algorithm for tagging.\n",
        "\n",
        "        Args:\n",
        "            read_emission_file: Whether to read the emission matrix from a file.\n",
        "\n",
        "        Returns:\n",
        "            List of tuples containing word-tag pairs.\n",
        "        \"\"\"\n",
        "        viterbi = []\n",
        "        for key, word in enumerate(self.test_tagged_words):\n",
        "            max_prob = -1\n",
        "            best_tag = None\n",
        "            for tag in self.tagset:\n",
        "                if key == 0:\n",
        "                    transition_p = self.trans_df.loc['.', tag]\n",
        "                else:\n",
        "                    last_state = viterbi[-1]\n",
        "                    transition_p = self.trans_df.loc[last_state, tag]\n",
        "\n",
        "                emission_p = self.emission_df.iat[key, self.tagset.index(tag)]\n",
        "\n",
        "                prob = emission_p * transition_p\n",
        "                if prob >= max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_tag = tag\n",
        "\n",
        "            viterbi.append(best_tag)\n",
        "        return list(zip(self.test_tagged_words, viterbi))\n",
        "\n",
        "    def smoothed_viterbi(self) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Perform the Viterbi algorithm with smoothing for tagging.\n",
        "\n",
        "        Returns:\n",
        "            List of tuples containing word-tag pairs.\n",
        "        \"\"\"\n",
        "        viterbi = []\n",
        "        for key, word in enumerate(self.test_tagged_words):\n",
        "            max_prob = -1\n",
        "            best_tag = None\n",
        "            for tag in self.tagset:\n",
        "                if key == 0:\n",
        "                    transition_p = self.trans_df.loc['.', tag]\n",
        "                else:\n",
        "                    last_state = viterbi[-1]\n",
        "                    transition_p = self.trans_df.loc[last_state, tag]\n",
        "\n",
        "                if word in self.vocab:\n",
        "                    emission_p = self.emission_df.iat[key, self.tagset.index(tag)]\n",
        "                else:\n",
        "                    emission_p = self.smoothing_const\n",
        "\n",
        "                prob = emission_p * transition_p\n",
        "                if prob >= max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_tag = tag\n",
        "\n",
        "            viterbi.append(best_tag)\n",
        "        return list(zip(self.test_tagged_words, viterbi))\n",
        "\n",
        "    def apply_viterbi(self, read_emission_matrix: bool = True, smoothing: bool = True) -> List[Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Apply the Viterbi algorithm for tagging.\n",
        "\n",
        "        Args:\n",
        "            read_emission_matrix: Whether to read the emission matrix from a file.\n",
        "            smoothing: Whether to use smoothing during tagging.\n",
        "\n",
        "        Returns:\n",
        "            List of tuples containing word-tag pairs.\n",
        "        \"\"\"\n",
        "        self.set_transition_matrix()\n",
        "        self.set_emission_matrix(read_from_file=read_emission_matrix)\n",
        "        if smoothing:\n",
        "            tagged_res = self.smoothed_viterbi()\n",
        "        else:\n",
        "            tagged_res = self._viterbi()\n",
        "        return tagged_res\n",
        "\n",
        "    def evaluate(self, tagged_res: List[Tuple[str, str]]) -> Tuple[float, List[Tuple[str, str]]]:\n",
        "        \"\"\"\n",
        "        Evaluate the accuracy of the tagging results.\n",
        "\n",
        "        Args:\n",
        "            tagged_res: List of tuples containing word-tag pairs.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of accuracy and a list of wrong predictions.\n",
        "        \"\"\"\n",
        "        res = []\n",
        "        wrong_preds = []\n",
        "        for i, j in zip(tagged_res, self.test_run_base):\n",
        "            if i == j:\n",
        "                res.append(1)\n",
        "\n",
        "        for _, pair in enumerate(zip(tagged_res, self.test_run_base)):\n",
        "            if pair[0] != pair[1]:\n",
        "                wrong_preds.append(pair)\n",
        "\n",
        "        accuracy = len(res) / len(tagged_res)\n",
        "        return accuracy, wrong_preds\n"
      ],
      "metadata": {
        "id": "x4ZKH14JaPVd"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}